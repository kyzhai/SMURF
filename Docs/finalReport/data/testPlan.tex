\section{Test Plan}

During the development process of SMURF, to let everyone involve in the development as much as possible, we adopt the slice model, 
i.e., in each developing stage, everyone has a slice of assignment to work on.
One problem with this model is that different people needs to work on a same job, 
so one people's change to the program can easily crash other people's work. 
As a result, extensive tests to ensure the quality of the software is crucial. 

\subsection{Testing Levels}
\subsubsection{Unit Testing}
For lexer and parser, we generated a separate executable to test their functionality (parser_test.ml). 
This executable reads in SMURF programs, analyzes the input files with lexer and parser, generates the abstract syntax trees, 
 and then spits out the information stored in the ast trees.

\subsubsection{Integration Testing}
We tested the correctness of semantic checking and code generation models together with the lexer and parser models.
We generated the toplevel executable with semantic_test.ml for semantic checking, and with toplevel.ml for code generation. 
The output of semantic checking is the semantic abstract syntax tree, 
which is the abstract syntax tree with types of every variables resolved.
The output of code generation is the bytecode for midi music generation.

We also tested the integration between our bytecode and the midi music generator in java.

\subsubsection{System Testing}
The end-to-end SMURF compiler accepts SMURF program and generates MIDIs, with the bytecode for MIDI generation as byproduct.
For the ASCII bytecode we compare it with golden results to make sure its correctness. 
We also listen to the MIDIs generated by SMURF with music players to make the sounds are correct.

\subsection{Test Suits}

The hierarchy for SMURF test cases is shown in (figure~\ref{fig:testDir}). 
In each developing stage, everyone is in charge of a directory holding test cases constructed for the functionality he/she is working on. 
Every person needs to give the expecting output for his/her test cases in the {\bf exp} directory.
A case passes the test if its output is identical with the corresponding output in the {\bf exp} directory.
We have a script for testing all the test cases in the toplevel of the directory running all the test cases and comparing the results with the expect results given by every owner of the cases. 
The script gives the result about how many test cases passed and which test cases failed, if any. 
Before committing his/her result to the repository, everyone need to make sure the new change passed all the other people's cases. 
For the occasions that one people's work need to change the output of other people's cases, 
he/she need to check the changes are as expected, 
and then generate new expected results for the cases before committing changes to repository.

There are two types of test cases, to pass and to fail. We don't treat them differently during the test.
As long as the program is not broken, on one hand, the pass cases should give the same output, 
On the other hand, the fail cases should give the same exception message as that stored in {\bf exp} directory.
We use the convention to name the fail cases starting with {\it test-fail-} and name the pass cases starting with {\it test-}.

\begin{figure} [H]
\centering
\begin{tikzpicture}[%
grow via three points={one child at (0.5,-0.7) and
    two children at (0.5,-0.7) and (0.5,-1.4)},
    edge from parent path={(\tikzparentnode.south) |- (\tikzchildnode.west)}]

    \node {TEST_ROOT}
    child { node {parser-tests}}     
    child { node {semantic-tests}}
    child { node [selected] {codegen-tests}
        child { node [selected] {person1}
            child{ node {exp}
                child{ node {test-chord.out} }
                child{ node {test-system.out} }
                child{ node {test-fail-note.out} }
                child{ node [optional] {...} }
            }
            child [missing] {}              
            child [missing] {}              
            child [missing] {}              
            child [missing] {}              
            child{ node {test-chord.sm} }
            child{ node {test-system.sm} }
            child{ node {test-fail-note.sm} }
            child{ node [optional] {...} }
        }
        child [missing] {}              
        child [missing] {}              
        child [missing] {}              
        child [missing] {}              
        child [missing] {}              
        child [missing] {}              
        child [missing] {}              
        child [missing] {}              
        child [missing] {}              
        child { node {person2}}
        child { node {person3}}
        child { node {person4}}
        child { node {person5}}
    };
\end{tikzpicture}
\caption{The directory of SMURF test cases}
\label{fig:testDir}
\end{figure}

\subsection{Example Test Cases}
Below we give several sample test cases and their expected output for SMURF.

\subsubsection{parser-tests}
Note that the programs that pass parser testing may not be semantically correct.
\lstinputlisting[title=test-if.sm]{../../Code/tests/parser-tests/kyzhai/test-if.sm}

\subsubsection{semantic-tests}
For cases successfully passed semantic checking, semantic checking spits out the semantic abstract syntax tree with the type of each variable resolved.
\lstinputlisting[title=test-emptyList.sm]{../../Code/tests/semantic-tests/kyzhai/test-emptyList.sm}
\lstinputlisting[title=test-emptyList.out]{../../Code/tests/semantic-tests/kyzhai/exp/test-emptyList.out}

For cases that failed to passed the semantic checking, 
semantic checking captures the exception and spits out the message regarding the exception 
to help the programmer to easily locate the problem in program.
\lstinputlisting[title=test-fail-chord.sm]{../../Code/tests/semantic-tests/kyzhai/test-fail-chord.sm}
\lstinputlisting[title=test-fail-chord.out]{../../Code/tests/semantic-tests/kyzhai/exp/test-fail-chord.out}

\subsubsection{codegen-tests}
Following is an example used for codegen test. 
\lstinputlisting[title=cascade.sm]{../../Code/tests/interp-tests/rtownsend/cascade.sm}
And following is the output bytecode generated by SMURF program.
\lstinputlisting[title=cascade.sm]{res/cascade.csv}


